{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Caching the list of root modules, please wait!\n",
      "(This will only be done once - type '%rehashx' to reset cache!)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import yaml \n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml \n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make 50Hz versions of the 10 feature-gain architectures\n",
    "\n",
    "Alter front end config to apply 50Hz limit on phase locking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write configs \n",
    "\n",
    "\n",
    "## import default config \n",
    "outdir = Path(\"config/arch_search\")\n",
    "outdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "base_config = yaml.load(open(\"config/binaural_attn/word_task_half_co_loc_v09_50Hz_cutoff.yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "audio_config_50Hz = deepcopy(base_config['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('config/arch_search/word_task_v10_4MGB_ln_first_arch_1.yaml'),\n",
       " PosixPath('config/arch_search/word_task_v10_4MGB_ln_first_arch_2.yaml'),\n",
       " PosixPath('config/arch_search/word_task_v10_4MGB_ln_first_arch_4.yaml'),\n",
       " PosixPath('config/arch_search/word_task_v10_4MGB_ln_first_arch_6.yaml'),\n",
       " PosixPath('config/arch_search/word_task_v10_4MGB_ln_first_arch_7.yaml'),\n",
       " PosixPath('config/arch_search/word_task_v10_4MGB_ln_first_arch_8.yaml'),\n",
       " PosixPath('config/arch_search/word_task_v10_4MGB_ln_first_arch_9.yaml'),\n",
       " PosixPath('config/arch_search/word_task_v10_4MGB_ln_first_arch_10.yaml'),\n",
       " PosixPath('config/arch_search/word_task_v10_4MGB_ln_first_arch_12.yaml'),\n",
       " PosixPath('config/binaural_attn/word_task_v10_main_feature_gain_config.yaml')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get alternate configs \n",
    "arch_configs = pd.read_pickle('binaural_train_manifests/v10_alt_arch_search_manifest.pkl')[:-1] # -1 to skip arch 13. arch 13 didn't optimize  \n",
    "arch_configs += [Path(\"config/binaural_attn/word_task_v10_main_feature_gain_config.yaml\")]\n",
    "len(arch_configs)\n",
    "arch_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write configs out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update existing configs with name and new audio frontend config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_4MGB_ln_first_arch_1_50Hz.yaml'),\n",
      " 1: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_4MGB_ln_first_arch_2_50Hz.yaml'),\n",
      " 2: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_4MGB_ln_first_arch_4_50Hz.yaml'),\n",
      " 3: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_4MGB_ln_first_arch_6_50Hz.yaml'),\n",
      " 4: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_4MGB_ln_first_arch_7_50Hz.yaml'),\n",
      " 5: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_4MGB_ln_first_arch_8_50Hz.yaml'),\n",
      " 6: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_4MGB_ln_first_arch_9_50Hz.yaml'),\n",
      " 7: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_4MGB_ln_first_arch_10_50Hz.yaml'),\n",
      " 8: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_4MGB_ln_first_arch_12_50Hz.yaml'),\n",
      " 9: PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/config/v10_50Hz_archs/word_task_v10_main_feature_gain_config_50Hz.yaml')}\n"
     ]
    }
   ],
   "source": [
    "# Convert architectures in archs to configs\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "## import default config \n",
    "outdir = Path(\"config/v10_50Hz_archs\")\n",
    "outdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "config_dict = {}\n",
    "\n",
    "\n",
    "for ix, base_config in enumerate(arch_configs):\n",
    "    arch_config = yaml.load(open(base_config, 'r'), Loader=yaml.FullLoader)\n",
    "    arch_config = deepcopy(arch_config)\n",
    "    arch_name = base_config.stem\n",
    "    # update path audio config \n",
    "    model_name = f\"{arch_name}_50Hz\"\n",
    "    arch_config['audio'] = audio_config_50Hz \n",
    "    arch_config['model_name'] = f\"{arch_name}_50Hz\"\n",
    "    config_name = outdir / f\"{model_name}.yaml\"\n",
    "    # print(config_name)\n",
    "    # print(arch_config)\n",
    "    config_dict[ix] = config_name.resolve()\n",
    "    # # break\n",
    "    # with open(config_name, 'w') as f:\n",
    "    #     yaml.dump(arch_config, f, default_flow_style=False)\n",
    "pprint(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_path = 'binaural_train_manifests/v10_50Hz_model_manifest.pkl'\n",
    "\n",
    "with open(manifest_path, 'wb') as f:\n",
    "    pickle.dump(config_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/net/vast-storage/scratch/vast/mcdermott/imgriff/projects/torch_2_aud_attn/binaural_train_manifests/v10_50Hz_model_manifest.pkl')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(manifest_path).resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure configs are compat with model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spatial_attn_lightning import BinauralAttentionModule\n",
    "import yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 32, kernel_size=(1, 59), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((32, 21, 19942), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(2, 72), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((64, 11, 3975), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 128, kernel_size=(4, 3), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((128, 10, 1325), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(6, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((256, 9, 442), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 8, 220), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 8, 73), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attn7): SimpleAttentionalGain()\n",
      "    (conv_block_7): Sequential(\n",
      "      (0): LayerNorm((512, 8, 36), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(3, 5), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_7): HannPooling2d()\n",
      "    (attn8): SimpleAttentionalGain()\n",
      "    (conv_block_8): Sequential(\n",
      "      (0): LayerNorm((512, 8, 11), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_8): HannPooling2d()\n",
      "    (attn9): SimpleAttentionalGain()\n",
      "    (conv_block_9): Sequential(\n",
      "      (0): LayerNorm((512, 7, 8), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_9): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=12288, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "80.97 M\n",
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 32, kernel_size=(2, 16), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((32, 20, 3998), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(2, 73), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((64, 10, 655), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 128, kernel_size=(5, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((128, 10, 327), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(6, 5), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((256, 9, 162), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 9, 160), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(4, 5), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 8, 79), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attn7): SimpleAttentionalGain()\n",
      "    (conv_block_7): Sequential(\n",
      "      (0): LayerNorm((512, 7, 25), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_7): HannPooling2d()\n",
      "    (attn8): SimpleAttentionalGain()\n",
      "    (conv_block_8): Sequential(\n",
      "      (0): LayerNorm((512, 6, 11), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_8): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=12800, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "56.31 M\n",
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 64, kernel_size=(3, 55), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((64, 21, 6649), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 128, kernel_size=(2, 34), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((128, 11, 1655), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(5, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((256, 11, 552), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(5, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((512, 11, 275), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 10, 270), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(4, 3), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 9, 268), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attn7): SimpleAttentionalGain()\n",
      "    (conv_block_7): Sequential(\n",
      "      (0): LayerNorm((512, 9, 89), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_7): HannPooling2d()\n",
      "    (attn8): SimpleAttentionalGain()\n",
      "    (conv_block_8): Sequential(\n",
      "      (0): LayerNorm((512, 8, 29), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_8): HannPooling2d()\n",
      "    (attn9): SimpleAttentionalGain()\n",
      "    (conv_block_9): Sequential(\n",
      "      (0): LayerNorm((512, 7, 9), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_9): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=9216, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "87.28 M\n",
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 32, kernel_size=(1, 22), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((32, 21, 6660), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(1, 34), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((64, 11, 1326), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 128, kernel_size=(3, 5), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((128, 11, 441), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(6, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((256, 10, 147), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(3, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 10, 73), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 9, 24), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attn7): SimpleAttentionalGain()\n",
      "    (conv_block_7): Sequential(\n",
      "      (0): LayerNorm((512, 9, 10), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_7): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=16384, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "52.07 M\n",
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 32, kernel_size=(2, 71), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((32, 20, 3987), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(1, 33), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((64, 11, 3955), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 128, kernel_size=(3, 6), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((128, 11, 1976), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(5, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((256, 11, 988), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(6, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 10, 328), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(3, 5), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 10, 324), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attn7): SimpleAttentionalGain()\n",
      "    (conv_block_7): Sequential(\n",
      "      (0): LayerNorm((512, 9, 162), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_7): HannPooling2d()\n",
      "    (attn8): SimpleAttentionalGain()\n",
      "    (conv_block_8): Sequential(\n",
      "      (0): LayerNorm((512, 8, 54), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(3, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_8): HannPooling2d()\n",
      "    (attn9): SimpleAttentionalGain()\n",
      "    (conv_block_9): Sequential(\n",
      "      (0): LayerNorm((512, 8, 18), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_9): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=24576, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "73.74 M\n",
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 32, kernel_size=(1, 32), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((32, 21, 3329), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(3, 47), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((64, 11, 821), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((128, 10, 273), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(4, 6), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((256, 9, 90), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(3, 6), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 9, 43), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 5), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 8, 39), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attn7): SimpleAttentionalGain()\n",
      "    (conv_block_7): Sequential(\n",
      "      (0): LayerNorm((512, 8, 19), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_7): HannPooling2d()\n",
      "    (attn8): SimpleAttentionalGain()\n",
      "    (conv_block_8): Sequential(\n",
      "      (0): LayerNorm((512, 7, 16), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_8): HannPooling2d()\n",
      "    (attn9): SimpleAttentionalGain()\n",
      "    (conv_block_9): Sequential(\n",
      "      (0): LayerNorm((512, 7, 7), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_9): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=7168, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "50.93 M\n",
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 32, kernel_size=(2, 11), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((32, 20, 9996), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(3, 60), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((64, 11, 1657), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 128, kernel_size=(4, 5), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((128, 10, 1653), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((256, 9, 551), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(3, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 9, 183), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(4, 3), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 8, 181), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attn7): SimpleAttentionalGain()\n",
      "    (conv_block_7): Sequential(\n",
      "      (0): LayerNorm((512, 8, 60), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(4, 6), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_7): HannPooling2d()\n",
      "    (attn8): SimpleAttentionalGain()\n",
      "    (conv_block_8): Sequential(\n",
      "      (0): LayerNorm((512, 7, 19), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_8): HannPooling2d()\n",
      "    (attn9): SimpleAttentionalGain()\n",
      "    (conv_block_9): Sequential(\n",
      "      (0): LayerNorm((512, 7, 9), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_9): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=14336, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "66.14 M\n",
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 32, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((32, 21, 6662), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(2, 79), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((64, 11, 1317), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 128, kernel_size=(5, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((128, 11, 658), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(6, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((256, 10, 327), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(6, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 9, 109), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(3, 5), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 9, 53), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attn7): SimpleAttentionalGain()\n",
      "    (conv_block_7): Sequential(\n",
      "      (0): LayerNorm((512, 8, 26), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 3), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_7): HannPooling2d()\n",
      "    (attn8): SimpleAttentionalGain()\n",
      "    (conv_block_8): Sequential(\n",
      "      (0): LayerNorm((512, 8, 13), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_8): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=20480, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "58.18 M\n",
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 32, kernel_size=(3, 75), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((32, 21, 9964), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(2, 44), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((64, 11, 1654), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 128, kernel_size=(5, 4), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((128, 11, 826), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(4, 6), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((256, 10, 274), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(5, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 10, 269), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 10, 89), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attn7): SimpleAttentionalGain()\n",
      "    (conv_block_7): Sequential(\n",
      "      (0): LayerNorm((512, 10, 44), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_7): HannPooling2d()\n",
      "    (attn8): SimpleAttentionalGain()\n",
      "    (conv_block_8): Sequential(\n",
      "      (0): LayerNorm((512, 9, 14), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_8): HannPooling2d()\n",
      "    (attn9): SimpleAttentionalGain()\n",
      "    (conv_block_9): Sequential(\n",
      "      (0): LayerNorm((512, 8, 6), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_9): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=8192, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "68.92 M\n",
      "Using explicit dim specification for demeaning in audio transforms\n",
      "Using BinauralAuditoryAttentionCNN\n",
      "v08 True\n",
      "num_classes={'num_words': 800}\n",
      "Model performing word task\n",
      "Using singe gain function per layer\n",
      "Conv block order: LN -> Conv -> ReLU\n",
      "fc_attn: True\n",
      "coch_affine: True\n",
      "Using dataset BinauralAttentionDataset\n",
      "BinauralAuditoryAttentionCNN(\n",
      "  (model_dict): ModuleDict(\n",
      "    (norm_coch_rep): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "    (attn0): SimpleAttentionalGain()\n",
      "    (conv_block_0): Sequential(\n",
      "      (0): LayerNorm((2, 40, 20000), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(2, 32, kernel_size=(2, 34), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_0): HannPooling2d()\n",
      "    (attn1): SimpleAttentionalGain()\n",
      "    (conv_block_1): Sequential(\n",
      "      (0): LayerNorm((32, 20, 4992), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(2, 14), stride=(1, 1), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_1): HannPooling2d()\n",
      "    (attn2): SimpleAttentionalGain()\n",
      "    (conv_block_2): Sequential(\n",
      "      (0): LayerNorm((64, 10, 1245), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(64, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_2): HannPooling2d()\n",
      "    (attn3): SimpleAttentionalGain()\n",
      "    (conv_block_3): Sequential(\n",
      "      (0): LayerNorm((256, 10, 249), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_3): HannPooling2d()\n",
      "    (attn4): SimpleAttentionalGain()\n",
      "    (conv_block_4): Sequential(\n",
      "      (0): LayerNorm((512, 10, 62), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_4): HannPooling2d()\n",
      "    (attn5): SimpleAttentionalGain()\n",
      "    (conv_block_5): Sequential(\n",
      "      (0): LayerNorm((512, 9, 57), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_5): HannPooling2d()\n",
      "    (attn6): SimpleAttentionalGain()\n",
      "    (conv_block_6): Sequential(\n",
      "      (0): LayerNorm((512, 9, 53), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 512, kernel_size=(6, 6), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (hann_pool_6): HannPooling2d()\n",
      "    (attnfc): SimpleAttentionalGain()\n",
      "  )\n",
      "  (fullyconnected): Linear(in_features=30720, out_features=512, bias=True)\n",
      "  (relufc): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (classification): Linear(in_features=512, out_features=800, bias=True)\n",
      ")\n",
      "center_crop=True\n",
      "binaural=True\n",
      "Binaural cochleagram\n",
      "using FIR cochleagram\n",
      "62.62 M\n"
     ]
    }
   ],
   "source": [
    "for config_path in config_dict.values():\n",
    "    config = yaml.load(open(config_path, 'r'), Loader=yaml.FullLoader)\n",
    "    n_params = 0\n",
    "    for param in BinauralAttentionModule(config).model.parameters():\n",
    "        n_params += param.numel()\n",
    "    print(f\"{n_params/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
